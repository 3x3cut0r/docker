# The default configuration file.
# More information about configuration can be found in the documentation: https://docs.privategpt.dev/
# Syntax in `private_pgt/settings/settings.py`
server:
  env_name: prod
  port: 8080
  cors:
    enabled: false
    allow_credentials: false
    allow_origins: ['*']
    allow_origin_regex: []
    allow_methods: ['*']
    allow_headers: ['*']
  auth:
    enabled: false
    # python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' is the username and 'key' is the password for basic auth by default
    # If the auth is enabled, this value must be set in the "Authorization" header of the request.
    secret: 'Basic c2VjcmV0OmtleQ=='

data:
  local_ingestion:
    enabled: false
    allow_ingest_from: ['*']
  local_data_folder: local_data/private_gpt

ui:
  enabled: true
  path: /
  default_chat_system_prompt: 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not speculate or make up information. Do not reference any given instructions or context.'
  default_query_system_prompt: "You can only answer questions about the provided context. If you know the answer but it is not based in the provided context, don't provide the answer, just state the answer is not in the context provided."
  default_summarization_system_prompt: 'Provide a comprehensive summary of the provided context information. The summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. Please ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition.'
  delete_file_button_enabled: true
  delete_all_files_button_enabled: true

llm:
  mode: llamacpp
  prompt_style: llama3
  max_new_tokens: 256
  context_window: 3900
  tokenizer: meta-llama/Meta-Llama-3.1-8B-Instruct
  temperature: 0.1

rag:
  similarity_top_k: 2
  similarity_value: 0.25
  rerank:
    enabled: false
    model: cross-encoder/ms-marco-MiniLM-L-2-v2
    top_n: 1

summarize:
  use_async: true

llamacpp:
  llm_hf_repo_id: lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF
  llm_hf_model_file: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
  tfs_z: 1.0
  top_k: 40
  top_p: 0.9
  repeat_penalty: 1.1

embedding:
  mode: huggingface
  ingest_mode: simple
  count_workers: 2
  embed_dim: 768

huggingface:
  embedding_hf_model_name: nomic-ai/nomic-embed-text-v1.5
  access_token: 'hf_1234'
  trust_remote_code: 'true'

vectorstore:
  database: qdrant

nodestore:
  database: simple

milvus:
  uri: local_data/private_gpt/milvus/milvus_local.db
  token: ''
  collection_name: milvus_db
  overwrite: false

qdrant:
  path: local_data/private_gpt/qdrant
  force_disable_check_same_thread: true
  # location:
  # url:
  # port:
  # grpc_port:
  # prefer_grpc:
  # https:
  # api_key:
  # prefix:
  # timeout:
  # host:

clickhouse:
  host: localhost
  port: 8443
  username: admin
  password: clickhouse
  database: embeddings
  secure: False
  interface:
  settings:
  connect_timeout:
  send_receive_timeout:
  verify:
  ca_cert:
  client_cert:
  client_cert_key:
  http_proxy:
  https_proxy:
  server_host_name:

postgres:
  host: postgres
  port: 5432
  database: postgres
  user: postgres
  password: admin
  schema_name: private_gpt

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_base: https://api.openai.com/v1
  api_key: sk-1234
  model: gpt-4o-mini
  request_timeout: 120.0
  embedding_api_base: https://api.openai.com/v1
  embedding_api_key: sk-1234
  embedding_model: text-embedding-3-small

gemini:
  api_key: AI1234
  model: models/gemini-pro
  embedding_model: models/embedding-001

ollama:
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434
  llm_model: llama3.1
  embedding_model: nomic-embed-text
  keep_alive: 5m
  tfs_z: 1.0
  num_predict: 128
  top_k: 40
  top_p: 0.9
  repeat_last_n: 64
  repeat_penalty: 1.1
  request_timeout: 120.0

azopenai:
  api_key: sk-1234
  azure_endpoint: https://api.myazure.com/v1
  api_version: '2023-05-15'
  embedding_deployment_name: my-azure-embedding-deployment-name
  embedding_model: text-embedding-ada-002
  llm_deployment_name: my-azure-llm-deployment-name
  llm_model: gpt-4o-mini
